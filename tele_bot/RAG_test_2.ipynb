{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue, Range\n",
    "import rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from mistralai import Mistral\n",
    "\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "QDRANT_COLLECTION_EMBEDDINGS = \"telegram_embeddings\"\n",
    "QDRANT_COLLECTION_BM25 = \"telegram_bm25\"\n",
    "VECTOR_SIZE = 384\n",
    "\n",
    "class TelegramRAGSystem:\n",
    "    def __init__(self, data_dir: str = \"./rag_data\", mistral_api_key: str = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant\n",
    "        self.qdrant_embeddings = QdrantClient(path=os.path.join(data_dir, \"embeddings_db\"))\n",
    "        self.qdrant_bm25 = QdrantClient(path=os.path.join(data_dir, \"bm25_db\"))\n",
    "        \n",
    "        # BM25\n",
    "        self.bm25 = None\n",
    "        self.bm25_documents = []\n",
    "        self.bm25_doc_ids = []  # –•—Ä–∞–Ω–∏–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ ID\n",
    "        self.bm25_uuid_map = {}  # –ú–∞–ø–ø–∏–Ω–≥ UUID -> –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π ID\n",
    "        \n",
    "        # Mistral –∫–ª–∏–µ–Ω—Ç\n",
    "        self.mistral_client = Mistral(api_key=mistral_api_key) if mistral_api_key else None\n",
    "        \n",
    "        self._initialize_collections()\n",
    "        self._load_bm25_data()\n",
    "    \n",
    "    def _generate_uuid(self, original_id: str) -> str:\n",
    "        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è UUID –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ ID\"\"\"\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, original_id))\n",
    "    \n",
    "    def _initialize_collections(self):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant\"\"\"\n",
    "        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        if not self.qdrant_embeddings.collection_exists(QDRANT_COLLECTION_EMBEDDINGS):\n",
    "            self.qdrant_embeddings.create_collection(\n",
    "                collection_name=QDRANT_COLLECTION_EMBEDDINGS,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "        \n",
    "        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è BM25\n",
    "        if not self.qdrant_bm25.collection_exists(QDRANT_COLLECTION_BM25):\n",
    "            self.qdrant_bm25.create_collection(\n",
    "                collection_name=QDRANT_COLLECTION_BM25,\n",
    "                vectors_config=VectorParams(size=1, distance=Distance.COSINE)\n",
    "            )\n",
    "    \n",
    "    def _load_bm25_data(self):\n",
    "        \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è BM25 –∏–∑ Qdrant\"\"\"\n",
    "        points = self.qdrant_bm25.scroll(\n",
    "            collection_name=QDRANT_COLLECTION_BM25,\n",
    "            limit=10000\n",
    "        )[0]\n",
    "        \n",
    "        documents = []\n",
    "        doc_ids = []\n",
    "        \n",
    "        for point in points:\n",
    "            if point.payload and 'text' in point.payload and 'original_id' in point.payload:\n",
    "                documents.append(self._tokenize_text(point.payload['text']))\n",
    "                doc_ids.append(point.payload['original_id'])\n",
    "                self.bm25_uuid_map[point.id] = point.payload['original_id']\n",
    "        \n",
    "        if documents:\n",
    "            self.bm25 = BM25Okapi(documents)\n",
    "            self.bm25_documents = documents\n",
    "            self.bm25_doc_ids = doc_ids\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è BM25\n",
    "        Args:\n",
    "            text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "        Returns:\n",
    "            List[str]: —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (–æ—Å–Ω–æ–≤ —Å–ª–æ–≤)\n",
    "        \"\"\"\n",
    "        self.stemmer = SnowballStemmer('russian')\n",
    "        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤ (–±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –¥–µ—Ñ–∏—Å—ã, –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã)\n",
    "        self.word_pattern = re.compile(r\"[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]+(?:[-'‚Äô][a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]+)*\")\n",
    "        \n",
    "        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        text = text.lower()\n",
    "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n",
    "        words = self.word_pattern.findall(text)\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–µ–º–º–∏–Ω–≥ –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É\n",
    "        tokens = [self.stemmer.stem(word) for word in words]\n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–±–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "        \"\"\"\n",
    "        points_embeddings = []\n",
    "        points_bm25 = []\n",
    "        new_bm25_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è UUID\n",
    "            doc_uuid = self._generate_uuid(doc['id'])\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞\n",
    "            embedding = self.embedding_model.encode(doc['text']).tolist()\n",
    "            \n",
    "            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload\n",
    "            payload = {\n",
    "                'text': doc['text'],\n",
    "                'user_id': doc['user_id'],\n",
    "                'timestamp': doc['timestamp'],\n",
    "                'original_id': doc['id']  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π ID\n",
    "            }\n",
    "            \n",
    "            points_embeddings.append(PointStruct(\n",
    "                id=doc_uuid,\n",
    "                vector=embedding,\n",
    "                payload=payload\n",
    "            ))\n",
    "            \n",
    "            points_bm25.append(PointStruct(\n",
    "                id=doc_uuid,\n",
    "                vector=[1.0],\n",
    "                payload=payload\n",
    "            ))\n",
    "            \n",
    "            new_bm25_docs.append(self._tokenize_text(doc['text']))\n",
    "            self.bm25_uuid_map[doc_uuid] = doc['id']\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ Qdrant\n",
    "        if points_embeddings:\n",
    "            self.qdrant_embeddings.upsert(\n",
    "                collection_name=QDRANT_COLLECTION_EMBEDDINGS,\n",
    "                points=points_embeddings\n",
    "            )\n",
    "        \n",
    "        if points_bm25:\n",
    "            self.qdrant_bm25.upsert(\n",
    "                collection_name=QDRANT_COLLECTION_BM25,\n",
    "                points=points_bm25\n",
    "            )\n",
    "            \n",
    "            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25\n",
    "            if self.bm25 is None:\n",
    "                self.bm25 = BM25Okapi(new_bm25_docs)\n",
    "                self.bm25_documents = new_bm25_docs\n",
    "                self.bm25_doc_ids = [doc['id'] for doc in documents]\n",
    "            else:\n",
    "                self.bm25_documents.extend(new_bm25_docs)\n",
    "                self.bm25_doc_ids.extend([doc['id'] for doc in documents])\n",
    "                self.bm25 = BM25Okapi(self.bm25_documents)\n",
    "    \n",
    "    def recalculate_bm25(self):\n",
    "        \"\"\"–ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ—Å—á–µ—Ç BM25 –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –≤ –±–∞–∑–µ\"\"\"\n",
    "        self._load_bm25_data()\n",
    "    \n",
    "    def _build_filters(self, user_id: Optional[str] = None, \n",
    "                      start_timestamp: Optional[float] = None, \n",
    "                      end_timestamp: Optional[float] = None) -> Optional[Filter]:\n",
    "        \"\"\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è Qdrant\"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        if user_id:\n",
    "            conditions.append(FieldCondition(\n",
    "                key=\"user_id\",\n",
    "                match=MatchValue(value=user_id)\n",
    "            ))\n",
    "        \n",
    "        if start_timestamp is not None or end_timestamp is not None:\n",
    "            timestamp_range = {}\n",
    "            if start_timestamp is not None:\n",
    "                timestamp_range[\"gte\"] = start_timestamp\n",
    "            if end_timestamp is not None:\n",
    "                timestamp_range[\"lte\"] = end_timestamp\n",
    "            \n",
    "            conditions.append(FieldCondition(\n",
    "                key=\"timestamp\",\n",
    "                range=Range(**timestamp_range)\n",
    "            ))\n",
    "        \n",
    "        return Filter(must=conditions) if conditions else None\n",
    "    \n",
    "    def search(self, query: str, k: int = 10, m: int = 50,\n",
    "               user_id: Optional[str] = None,\n",
    "               start_timestamp: Optional[float] = None,\n",
    "               end_timestamp: Optional[float] = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        –ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RRF\n",
    "        \"\"\"\n",
    "        filters = self._build_filters(user_id, start_timestamp, end_timestamp)\n",
    "        \n",
    "        # –ü–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º\n",
    "        query_embedding = self.embedding_model.encode(query).tolist()\n",
    "        embedding_results = self.qdrant_embeddings.search(\n",
    "            collection_name=QDRANT_COLLECTION_EMBEDDINGS,\n",
    "            query_vector=query_embedding,\n",
    "            query_filter=filters,\n",
    "            limit=m\n",
    "        )\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º UUID –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ ID –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        embedding_results_converted = []\n",
    "        for result in embedding_results:\n",
    "            original_id = result.payload.get('original_id', result.id)\n",
    "            embedding_results_converted.append((original_id, result.score))\n",
    "        \n",
    "        # –ü–æ–∏—Å–∫ –ø–æ BM25\n",
    "        bm25_results = []\n",
    "        if self.bm25:\n",
    "            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π\n",
    "            all_points = self.qdrant_bm25.scroll(\n",
    "                collection_name=QDRANT_COLLECTION_BM25,\n",
    "                scroll_filter=filters,\n",
    "                limit=10000\n",
    "            )[0]\n",
    "            \n",
    "            filtered_docs = []\n",
    "            filtered_original_ids = []\n",
    "            \n",
    "            for point in all_points:\n",
    "                original_id = point.payload.get('original_id')\n",
    "                if original_id and original_id in self.bm25_doc_ids:\n",
    "                    idx = self.bm25_doc_ids.index(original_id)\n",
    "                    filtered_docs.append(self.bm25_documents[idx])\n",
    "                    filtered_original_ids.append(original_id)\n",
    "            \n",
    "            if filtered_docs:\n",
    "                temp_bm25 = BM25Okapi(filtered_docs)\n",
    "                tokenized_query = self._tokenize_text(query)\n",
    "                bm25_scores = temp_bm25.get_scores(tokenized_query)\n",
    "                \n",
    "                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score\n",
    "                bm25_indices = np.argsort(bm25_scores)[::-1][:m]\n",
    "                bm25_results = [(filtered_original_ids[i], float(bm25_scores[i])) \n",
    "                               for i in bm25_indices if bm25_scores[i] > 0]\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å RRF\n",
    "        return self._rrf_fusion(embedding_results_converted, bm25_results, k=k)\n",
    "    \n",
    "    def _rrf_fusion(self, embedding_results: List, bm25_results: List, k: int = 10, k_rrf: int = 60) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Reciprocal Rank Fusion\n",
    "        \"\"\"\n",
    "        ranked_lists = []\n",
    "        \n",
    "        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        if embedding_results:\n",
    "            embedding_rank = {doc_id: rank for rank, (doc_id, _) in enumerate(embedding_results)}\n",
    "            ranked_lists.append(embedding_rank)\n",
    "        \n",
    "        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è BM25\n",
    "        if bm25_results:\n",
    "            bm25_rank = {doc_id: rank for rank, (doc_id, _) in enumerate(bm25_results)}\n",
    "            ranked_lists.append(bm25_rank)\n",
    "        \n",
    "        # RRF –ø–æ–¥—Å—á–µ—Ç\n",
    "        rrf_scores = {}\n",
    "        for ranking in ranked_lists:\n",
    "            for doc_id, rank in ranking.items():\n",
    "                if doc_id not in rrf_scores:\n",
    "                    rrf_scores[doc_id] = 0\n",
    "                rrf_scores[doc_id] += 1.0 / (k_rrf + rank + 1)\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ RRF score\n",
    "        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return sorted_results\n",
    "    \n",
    "    def delete_documents(self, user_id: Optional[str] = None,\n",
    "                        start_timestamp: Optional[float] = None,\n",
    "                        end_timestamp: Optional[float] = None,\n",
    "                        doc_ids: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º\n",
    "        \"\"\"\n",
    "        filters = None\n",
    "        if user_id or start_timestamp is not None or end_timestamp is not None:\n",
    "            filters = self._build_filters(user_id, start_timestamp, end_timestamp)\n",
    "        \n",
    "        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ ID, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ UUID\n",
    "        uuid_ids = None\n",
    "        if doc_ids:\n",
    "            uuid_ids = [self._generate_uuid(doc_id) for doc_id in doc_ids]\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –æ–±–µ–∏—Ö –±–∞–∑\n",
    "        self.qdrant_embeddings.delete(\n",
    "            collection_name=QDRANT_COLLECTION_EMBEDDINGS,\n",
    "            points_selector=uuid_ids if uuid_ids else filters\n",
    "        )\n",
    "        \n",
    "        self.qdrant_bm25.delete(\n",
    "            collection_name=QDRANT_COLLECTION_BM25,\n",
    "            points_selector=uuid_ids if uuid_ids else filters\n",
    "        )\n",
    "        \n",
    "        # –ü–µ—Ä–µ—Å—á–µ—Ç BM25 –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è\n",
    "        self.recalculate_bm25()\n",
    "    \n",
    "    def get_document_texts(self, doc_ids: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∏—Ö ID\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ ID –≤ UUID –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "        uuid_ids = [self._generate_uuid(doc_id) for doc_id in doc_ids]\n",
    "        \n",
    "        points = self.qdrant_bm25.retrieve(\n",
    "            collection_name=QDRANT_COLLECTION_BM25,\n",
    "            ids=uuid_ids\n",
    "        )\n",
    "        \n",
    "        for point in points:\n",
    "            if point and point.payload:\n",
    "                original_id = point.payload.get('original_id', point.id)\n",
    "                results.append((original_id, point.payload['text']))\n",
    "        return results\n",
    "\n",
    "def run_mistral(messages, user_format=True, model=\"mistral-medium-latest\", api_key=None):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Mistral API\"\"\"\n",
    "    client = Mistral(api_key=api_key)\n",
    "    if user_format:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": messages}\n",
    "        ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "def user_message(inquiry, messages=[]):\n",
    "    \"\"\"\n",
    "    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º—Ç–∞ –¥–ª—è LLM\n",
    "    \"\"\"\n",
    "    formatted_messages = \"\\n\".join([f\"<{{{msg[0]}, {msg[1]}}}>\" for msg in messages]) if messages else \"–ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π.\"\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "–†–û–õ–¨: –¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —á–∞—Ç–∞—Ö Telegram.\n",
    "–ó–ê–î–ê–ß–ê: –ù–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç.\n",
    "–§–û–†–ú–ê–¢ –î–ê–ù–ù–´–•:\n",
    "–ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n",
    "<{{message_id, message_text}}>\n",
    "–ö–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ.\n",
    "\n",
    "–ü–†–ê–í–ò–õ–ê:\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n",
    "- –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Ç–≤–µ—Ç—å \"–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.\"\n",
    "- –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫ –∏ —Ç–æ—á–µ–Ω.\n",
    "- –ù–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –≤—ã–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ.\n",
    "\n",
    "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:\n",
    "–û—Ç–≤–µ—Ç: [—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞]\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: <{{message_id1, message_id2, ...}}>\n",
    "\n",
    "–ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π:\n",
    "{formatted_messages}\n",
    "\n",
    "–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {inquiry}\n",
    "\"\"\"\n",
    "    return user_message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–ï–°–¢–´.\n",
    "\n",
    "–∫–∞—Å–ø–æ–º –≤—ã–¥–µ–ª–∏–ª –Ω–∞ —á—Ç–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/zdfk35s17rg427bmt4dyz37r0000gn/T/ipykernel_88339/2858080996.py:194: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  embedding_results = self.qdrant_embeddings.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: [('msg_004', 0.03278688524590164), ('msg_003', 0.0315136476426799), ('msg_006', 0.016129032258064516), ('msg_001', 0.015873015873015872), ('msg_002', 0.015625)]\n",
      "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: [('msg_004', '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤—Å—Ç—Ä–µ—á–∏?'), ('msg_003', '–û—Ç–º–µ–Ω–∞, –í—Å—Ç—Ä–µ—á–∞ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ —Å 15:00 –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫. —Ç–æ–≥–¥–∞ –∏ –∑–∞–∫–ª—é—á–∏–º —Å–¥–µ–ª–∫—É'), ('msg_006', '–ò–¥–µ–º –≥—É–ª—è—Ç—å —Å–µ–≥–æ–¥–Ω—è –≤ –ø–∞—Ä–∫'), ('msg_001', '–í—Å—Ç—Ä–µ—á–∞–µ–º—Å—è –∑–∞–≤—Ç—Ä–∞ –≤ 15:00 —É –≥–ª–∞–≤–Ω–æ–≥–æ –≤—Ö–æ–¥–∞'), ('msg_002', '–ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤–∑—è—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —Å–æ–±—Ä–∞–Ω–∏–µ')]\n",
      "–û—Ç–≤–µ—Ç LLM: –û—Ç–≤–µ—Ç: –°–µ–≥–æ–¥–Ω—è –º—ã –∏–¥–µ–º –≥—É–ª—è—Ç—å –≤ 15:00.\n",
      "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è: <{msg_004, –ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤—Å—Ç—Ä–µ—á–∏?}>\n",
      "<{msg_006, –ò–¥–µ–º –≥—É–ª—è—Ç—å —Å–µ–≥–æ–¥–Ω—è –≤ –ø–∞—Ä–∫}>\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def main():\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã\n",
    "    #–ü–û–ú–ò–û–¢–†–ï–¢–¨ –ö–ê–ö –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–¢–°–Ø –ë–î–®–öA (—á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π —Ä–∞–∑ –Ω–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å)\n",
    "    rag_system = TelegramRAGSystem(data_dir=\"./rag_data\", mistral_api_key=\"your_mistral_api_key\")\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    test_documents = [\n",
    "        {\n",
    "            'id': 'msg_001',\n",
    "            'text': '–í—Å—Ç—Ä–µ—á–∞–µ–º—Å—è –∑–∞–≤—Ç—Ä–∞ –≤ 15:00 —É –≥–ª–∞–≤–Ω–æ–≥–æ –≤—Ö–æ–¥–∞',\n",
    "            'user_id': 'user1',\n",
    "            'timestamp': 1672531200\n",
    "        },\n",
    "        {\n",
    "            'id': 'msg_002', \n",
    "            'text': '–ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤–∑—è—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —Å–æ–±—Ä–∞–Ω–∏–µ',\n",
    "            'user_id': 'user2',\n",
    "            'timestamp': 1672617600\n",
    "        },\n",
    "        {\n",
    "            'id': 'msg_003',\n",
    "            'text': '–û—Ç–º–µ–Ω–∞, –í—Å—Ç—Ä–µ—á–∞ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ —Å 15:00 –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫. —Ç–æ–≥–¥–∞ –∏ –∑–∞–∫–ª—é—á–∏–º —Å–¥–µ–ª–∫—É',\n",
    "            'user_id': 'user1',\n",
    "            'timestamp': 167200000\n",
    "        },\n",
    "        {\n",
    "            'id': 'msg_004',\n",
    "            'text': '–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –≤—Å—Ç—Ä–µ—á–∏?',\n",
    "            'user_id': 'user3',\n",
    "            'timestamp': 1672790400\n",
    "        },\n",
    "        {\n",
    "            'id': 'msg_005',\n",
    "            'text': '–í—Å—Ç—Ä–µ—á–∞–µ–º—Å—è –≤–µ—á–µ—Ä–æ–º —É –∫–ª—É–±–∞',\n",
    "            'user_id': 'user2',\n",
    "            'timestamp': 1673000000\n",
    "        },\n",
    "        {\n",
    "            'id': 'msg_006',\n",
    "            'text': '–ò–¥–µ–º –≥—É–ª—è—Ç—å —Å–µ–≥–æ–¥–Ω—è –≤ –ø–∞—Ä–∫',\n",
    "            'user_id': 'user1',\n",
    "            'timestamp': 1672963200\n",
    "        },\n",
    "\n",
    "\n",
    "    ]\n",
    "    rag_system.add_documents(test_documents)\n",
    "    \n",
    "    # –ü–û–°–ú–û–¢–†–ï–¢–¨ –§–û–†–ú–ê–¢–¨ –í–†–ï–ú–ï–ù–ò, –ü–û–î–¥–ï–†–ñ–ê–¢–¨ —Ñ–ò–õ–¢–†–ê–¶–∏—é –ü–û –í–†–ï–ú–ï–ù–ò, –£–ó–ï–†–£ (—Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç)\n",
    "    results = rag_system.search(\"–∫–æ–≥–¥–∞ –≤—Å—Ç—Ä–µ—á–∞?\", k=5, end_timestamp=1673000000-1)\n",
    "    print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:\", results)\n",
    "    \n",
    "    # –í–û–ó–ú–û–ñ–ù–û –°–¢–û–ò–¢ –ó–ê–í–ï–°–¢–ò –û–¢–î–ï–õ–¨–ù–£–Æ –ë–î–®–ö–£ –ü–û–î –¢–ï–ö–°–¢–ê\n",
    "    doc_ids = [doc_id for doc_id, score in results]\n",
    "    messages = rag_system.get_document_texts(doc_ids)\n",
    "    print(\"–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\", messages)\n",
    "    \n",
    "    \n",
    "    prompt = user_message(\"–≤–æ —Å–∫–æ–ª—å–∫–æ –∏–¥–µ–º —Ç—É—Å–∏—Ç—å?\", messages)\n",
    "    API_KEY = 'your_mistral_api_key'\n",
    "    #–ü–û–î–î–ï–†–ñ–ê–¢–¨ –†–ê–ó–ù–´–ï –ú–û–î–ï–õ–ò \n",
    "    response = run_mistral(prompt, api_key=API_KEY, model=\"mistral-tiny\")\n",
    "    print(\"–û—Ç–≤–µ—Ç LLM:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –º–∏—Å—Ç—Ä–∞–ª—é. –≤—ã–≤–æ–¥—è—Ç—Å—è –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the secret\n",
    "api_key = API_KEY\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå MISTRAL_API_KEY not found in Colab secrets!\")\n",
    "else:\n",
    "    print(\"‚úÖ API key loaded successfully from Colab secrets!\")\n",
    "\n",
    "# Initialize Mistral client\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "# Test connection\n",
    "def test_connection():\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        print(\"‚úÖ Connected successfully!\")\n",
    "        print(f\"Available models: {[m.id for m in models.data]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection failed: {e}\")\n",
    "        print(\"üí° If key is not active yet, wait a few minutes and try again\")\n",
    "\n",
    "test_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
